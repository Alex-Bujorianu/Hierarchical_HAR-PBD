{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# For use with Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/'My Drive'/'Colab Notebooks'/Hierarchical_HAR-PBD"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjacency matrix:  [[0. 1. 0. 0.]\n",
      " [1. 0. 1. 1.]\n",
      " [0. 1. 0. 1.]\n",
      " [0. 1. 1. 0.]]\n",
      "Classes in Y_train:  [ 0.  1.  2.  3.  4.  5.  6.  8. 13. 16. 17. 20. 21. 22. 23. 24. 25. 26.]\n",
      "Timestep:  120 Body num:  4 Num classes HAR:  27\n",
      "HARextend shape:  (None, 120, 4, 27)\n",
      "HAR temporal output1 shape:  (None, 27)\n",
      "PDB temporal output1 shape  (None, 2)\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1.]\n",
      "Class counts:  {0: 3906, 1: 2601, 2: 3450, 3: 198, 4: 690, 5: 291, 6: 378, 7: 1.0, 8: 3336, 9: 1.0, 10: 1.0, 11: 1.0, 12: 1.0, 13: 1284, 14: 1.0, 15: 1.0, 16: 1254, 17: 72, 18: 1.0, 19: 1.0, 20: 4320, 21: 219, 22: 795, 23: 4302, 24: 657, 25: 756, 26.0: 579}\n",
      "Epoch 1/100\n",
      "194/194 [==============================] - 36s 165ms/step - loss: 0.0679 - categorical_accuracy: 0.1245 - val_loss: 0.0544 - val_categorical_accuracy: 0.1575\n",
      "Epoch 2/100\n",
      "194/194 [==============================] - 28s 145ms/step - loss: 0.0578 - categorical_accuracy: 0.1400 - val_loss: 0.0525 - val_categorical_accuracy: 0.1590\n",
      "Epoch 3/100\n",
      "194/194 [==============================] - 30s 157ms/step - loss: 0.0552 - categorical_accuracy: 0.1444 - val_loss: 0.0518 - val_categorical_accuracy: 0.1641\n",
      "Epoch 4/100\n",
      "194/194 [==============================] - 30s 156ms/step - loss: 0.0526 - categorical_accuracy: 0.1582 - val_loss: 0.0516 - val_categorical_accuracy: 0.1669\n",
      "Epoch 5/100\n",
      "194/194 [==============================] - 30s 155ms/step - loss: 0.0507 - categorical_accuracy: 0.1714 - val_loss: 0.0514 - val_categorical_accuracy: 0.1488\n",
      "Epoch 6/100\n",
      "194/194 [==============================] - 32s 167ms/step - loss: 0.0496 - categorical_accuracy: 0.1695 - val_loss: 0.0522 - val_categorical_accuracy: 0.1427\n",
      "Epoch 7/100\n",
      "194/194 [==============================] - 31s 162ms/step - loss: 0.0480 - categorical_accuracy: 0.1841 - val_loss: 0.0522 - val_categorical_accuracy: 0.1422\n",
      "Epoch 8/100\n",
      "194/194 [==============================] - 31s 159ms/step - loss: 0.0472 - categorical_accuracy: 0.1923 - val_loss: 0.0522 - val_categorical_accuracy: 0.1354\n",
      "Epoch 9/100\n",
      "194/194 [==============================] - 31s 161ms/step - loss: 0.0466 - categorical_accuracy: 0.1987 - val_loss: 0.0528 - val_categorical_accuracy: 0.1615\n",
      "Epoch 10/100\n",
      "194/194 [==============================] - 31s 160ms/step - loss: 0.0458 - categorical_accuracy: 0.2022 - val_loss: 0.0523 - val_categorical_accuracy: 0.1615\n",
      "Epoch 11/100\n",
      "194/194 [==============================] - 33s 170ms/step - loss: 0.0450 - categorical_accuracy: 0.2044 - val_loss: 0.0534 - val_categorical_accuracy: 0.1623\n",
      "Epoch 12/100\n",
      "194/194 [==============================] - 32s 163ms/step - loss: 0.0445 - categorical_accuracy: 0.2092 - val_loss: 0.0520 - val_categorical_accuracy: 0.1488\n",
      "Epoch 13/100\n",
      "194/194 [==============================] - 31s 158ms/step - loss: 0.0437 - categorical_accuracy: 0.2133 - val_loss: 0.0524 - val_categorical_accuracy: 0.1491\n",
      "Epoch 14/100\n",
      "194/194 [==============================] - 32s 163ms/step - loss: 0.0435 - categorical_accuracy: 0.2130 - val_loss: 0.0530 - val_categorical_accuracy: 0.1478\n",
      "Epoch 15/100\n",
      "194/194 [==============================] - 31s 160ms/step - loss: 0.0432 - categorical_accuracy: 0.2155 - val_loss: 0.0531 - val_categorical_accuracy: 0.1434\n",
      "Epoch 16/100\n",
      "194/194 [==============================] - 31s 157ms/step - loss: 0.0428 - categorical_accuracy: 0.2160 - val_loss: 0.0527 - val_categorical_accuracy: 0.1448\n",
      "Epoch 17/100\n",
      "194/194 [==============================] - 30s 153ms/step - loss: 0.0424 - categorical_accuracy: 0.2224 - val_loss: 0.0535 - val_categorical_accuracy: 0.1444\n",
      "Epoch 18/100\n",
      "194/194 [==============================] - 30s 156ms/step - loss: 0.0419 - categorical_accuracy: 0.2200 - val_loss: 0.0533 - val_categorical_accuracy: 0.1425\n",
      "Epoch 19/100\n",
      "194/194 [==============================] - 31s 158ms/step - loss: 0.0420 - categorical_accuracy: 0.2241 - val_loss: 0.0541 - val_categorical_accuracy: 0.1468\n",
      "Epoch 20/100\n",
      "194/194 [==============================] - 30s 155ms/step - loss: 0.0415 - categorical_accuracy: 0.2236 - val_loss: 0.0543 - val_categorical_accuracy: 0.1444\n",
      "Epoch 21/100\n",
      "194/194 [==============================] - 29s 149ms/step - loss: 0.0416 - categorical_accuracy: 0.2219 - val_loss: 0.0549 - val_categorical_accuracy: 0.1499\n",
      "Epoch 22/100\n",
      "194/194 [==============================] - 30s 157ms/step - loss: 0.0411 - categorical_accuracy: 0.2280 - val_loss: 0.0550 - val_categorical_accuracy: 0.1454\n",
      "Epoch 23/100\n",
      "194/194 [==============================] - 30s 154ms/step - loss: 0.0410 - categorical_accuracy: 0.2256 - val_loss: 0.0547 - val_categorical_accuracy: 0.1439\n",
      "Epoch 24/100\n",
      "194/194 [==============================] - 30s 153ms/step - loss: 0.0410 - categorical_accuracy: 0.2266 - val_loss: 0.0553 - val_categorical_accuracy: 0.1490\n",
      "Epoch 25/100\n",
      "194/194 [==============================] - 30s 156ms/step - loss: 0.0407 - categorical_accuracy: 0.2298 - val_loss: 0.0557 - val_categorical_accuracy: 0.1495\n",
      "Epoch 26/100\n",
      "194/194 [==============================] - 36s 186ms/step - loss: 0.0405 - categorical_accuracy: 0.2341 - val_loss: 0.0559 - val_categorical_accuracy: 0.1516\n",
      "Epoch 27/100\n",
      "194/194 [==============================] - 33s 169ms/step - loss: 0.0404 - categorical_accuracy: 0.2302 - val_loss: 0.0545 - val_categorical_accuracy: 0.1428\n",
      "Epoch 28/100\n",
      "194/194 [==============================] - 32s 164ms/step - loss: 0.0401 - categorical_accuracy: 0.2349 - val_loss: 0.0554 - val_categorical_accuracy: 0.1462\n",
      "Epoch 29/100\n",
      "194/194 [==============================] - 31s 160ms/step - loss: 0.0400 - categorical_accuracy: 0.2352 - val_loss: 0.0558 - val_categorical_accuracy: 0.1481\n",
      "Epoch 30/100\n",
      "194/194 [==============================] - 31s 161ms/step - loss: 0.0398 - categorical_accuracy: 0.2388 - val_loss: 0.0553 - val_categorical_accuracy: 0.1456\n",
      "Epoch 31/100\n",
      "194/194 [==============================] - 32s 162ms/step - loss: 0.0396 - categorical_accuracy: 0.2358 - val_loss: 0.0550 - val_categorical_accuracy: 0.1474\n",
      "Epoch 32/100\n",
      "194/194 [==============================] - 31s 159ms/step - loss: 0.0395 - categorical_accuracy: 0.2382 - val_loss: 0.0558 - val_categorical_accuracy: 0.1420\n",
      "Epoch 33/100\n",
      "194/194 [==============================] - 31s 160ms/step - loss: 0.0395 - categorical_accuracy: 0.2425 - val_loss: 0.0565 - val_categorical_accuracy: 0.1504\n",
      "Epoch 34/100\n",
      "194/194 [==============================] - 32s 163ms/step - loss: 0.0392 - categorical_accuracy: 0.2416 - val_loss: 0.0556 - val_categorical_accuracy: 0.1425\n",
      "Epoch 35/100\n",
      "194/194 [==============================] - 32s 164ms/step - loss: 0.0391 - categorical_accuracy: 0.2422 - val_loss: 0.0559 - val_categorical_accuracy: 0.1453\n",
      "Epoch 36/100\n",
      "194/194 [==============================] - 31s 160ms/step - loss: 0.0389 - categorical_accuracy: 0.2433 - val_loss: 0.0562 - val_categorical_accuracy: 0.1467\n",
      "Epoch 37/100\n",
      "194/194 [==============================] - 33s 170ms/step - loss: 0.0389 - categorical_accuracy: 0.2431 - val_loss: 0.0571 - val_categorical_accuracy: 0.1484\n",
      "Epoch 38/100\n",
      "194/194 [==============================] - 32s 163ms/step - loss: 0.0388 - categorical_accuracy: 0.2415 - val_loss: 0.0550 - val_categorical_accuracy: 0.1437\n",
      "Epoch 39/100\n",
      "194/194 [==============================] - 30s 153ms/step - loss: 0.0386 - categorical_accuracy: 0.2423 - val_loss: 0.0558 - val_categorical_accuracy: 0.1456\n",
      "Epoch 40/100\n",
      "194/194 [==============================] - 29s 151ms/step - loss: 0.0384 - categorical_accuracy: 0.2483 - val_loss: 0.0570 - val_categorical_accuracy: 0.1490\n",
      "Epoch 41/100\n",
      "194/194 [==============================] - 29s 151ms/step - loss: 0.0385 - categorical_accuracy: 0.2501 - val_loss: 0.0563 - val_categorical_accuracy: 0.1533\n",
      "Epoch 42/100\n",
      "194/194 [==============================] - 30s 157ms/step - loss: 0.0383 - categorical_accuracy: 0.2470 - val_loss: 0.0560 - val_categorical_accuracy: 0.1532\n",
      "Epoch 43/100\n",
      "194/194 [==============================] - 31s 159ms/step - loss: 0.0382 - categorical_accuracy: 0.2490 - val_loss: 0.0561 - val_categorical_accuracy: 0.1501\n",
      "Epoch 44/100\n",
      " 49/194 [======>.......................] - ETA: 20s - loss: 0.0377 - categorical_accuracy: 0.2501"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [1], line 144\u001B[0m\n\u001B[1;32m    142\u001B[0m train \u001B[38;5;241m=\u001B[39m \u001B[38;5;28minput\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTrain model? Yes/no\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    143\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m train\u001B[38;5;241m==\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYes\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 144\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mHARmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY_test\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    145\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    146\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLoading model…\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn [1], line 132\u001B[0m, in \u001B[0;36mtrain_model\u001B[0;34m(model, X_train, X_test, Y_train, Y_test)\u001B[0m\n\u001B[1;32m    119\u001B[0m \u001B[38;5;66;03m# print(\"Shape of categorically encoded Y_train: \", Y_train.shape)\u001B[39;00m\n\u001B[1;32m    120\u001B[0m \u001B[38;5;66;03m# print(\"Shape of categorically encoded Y_test: \", Y_test.shape)\u001B[39;00m\n\u001B[1;32m    121\u001B[0m \u001B[38;5;66;03m# Beta = 0.9999 produces a really small loss\u001B[39;00m\n\u001B[1;32m    122\u001B[0m model\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mcompile(optimizer\u001B[38;5;241m=\u001B[39mAdam(learning_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5e-4\u001B[39m, decay\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-5\u001B[39m),\n\u001B[1;32m    123\u001B[0m               loss\u001B[38;5;241m=\u001B[39m{\n\u001B[1;32m    124\u001B[0m                     \u001B[38;5;66;03m# 'HARout': 'categorical_crossentropy'\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    129\u001B[0m               loss_weights\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mHARout\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m1.\u001B[39m},\n\u001B[1;32m    130\u001B[0m               metrics\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcategorical_accuracy\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m--> 132\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgraphtrain\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    133\u001B[0m \u001B[43m          \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mY_train\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    134\u001B[0m \u001B[43m          \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m150\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    135\u001B[0m \u001B[43m          \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    136\u001B[0m \u001B[43m          \u001B[49m\u001B[38;5;66;43;03m#callbacks=utils.build_callbacks('Model', str(valid_patient)),\u001B[39;49;00m\n\u001B[1;32m    137\u001B[0m \u001B[43m          \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mgraphtest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY_test\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    138\u001B[0m \u001B[43m          \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    139\u001B[0m model\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModels/GC_LSTM_HAR\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    140\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m model\u001B[38;5;241m.\u001B[39mmodel\n",
      "File \u001B[0;32m~/gits/venv/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     63\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/gits/venv/lib/python3.10/site-packages/keras/engine/training.py:1564\u001B[0m, in \u001B[0;36mModel.fit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m   1556\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mexperimental\u001B[38;5;241m.\u001B[39mTrace(\n\u001B[1;32m   1557\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   1558\u001B[0m     epoch_num\u001B[38;5;241m=\u001B[39mepoch,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1561\u001B[0m     _r\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[1;32m   1562\u001B[0m ):\n\u001B[1;32m   1563\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_begin(step)\n\u001B[0;32m-> 1564\u001B[0m     tmp_logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1565\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39mshould_sync:\n\u001B[1;32m   1566\u001B[0m         context\u001B[38;5;241m.\u001B[39masync_wait()\n",
      "File \u001B[0;32m~/gits/venv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/gits/venv/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:915\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    912\u001B[0m compiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnonXla\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    914\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile):\n\u001B[0;32m--> 915\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    917\u001B[0m new_tracing_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_get_tracing_count()\n\u001B[1;32m    918\u001B[0m without_tracing \u001B[38;5;241m=\u001B[39m (tracing_count \u001B[38;5;241m==\u001B[39m new_tracing_count)\n",
      "File \u001B[0;32m~/gits/venv/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:947\u001B[0m, in \u001B[0;36mFunction._call\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    944\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[1;32m    945\u001B[0m   \u001B[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001B[39;00m\n\u001B[1;32m    946\u001B[0m   \u001B[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001B[39;00m\n\u001B[0;32m--> 947\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_stateless_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# pylint: disable=not-callable\u001B[39;00m\n\u001B[1;32m    948\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stateful_fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    949\u001B[0m   \u001B[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001B[39;00m\n\u001B[1;32m    950\u001B[0m   \u001B[38;5;66;03m# in parallel.\u001B[39;00m\n\u001B[1;32m    951\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n",
      "File \u001B[0;32m~/gits/venv/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2495\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   2492\u001B[0m \u001B[38;5;124;03m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001B[39;00m\n\u001B[1;32m   2493\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[1;32m   2494\u001B[0m   (graph_function,\n\u001B[0;32m-> 2495\u001B[0m    filtered_flat_args) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_maybe_define_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2496\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m graph_function\u001B[38;5;241m.\u001B[39m_call_flat(\n\u001B[1;32m   2497\u001B[0m     filtered_flat_args, captured_inputs\u001B[38;5;241m=\u001B[39mgraph_function\u001B[38;5;241m.\u001B[39mcaptured_inputs)\n",
      "File \u001B[0;32m~/gits/venv/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2714\u001B[0m, in \u001B[0;36mFunction._maybe_define_function\u001B[0;34m(self, args, kwargs)\u001B[0m\n\u001B[1;32m   2690\u001B[0m \u001B[38;5;124;03m\"\"\"Gets a function for these inputs, defining it if necessary.\u001B[39;00m\n\u001B[1;32m   2691\u001B[0m \n\u001B[1;32m   2692\u001B[0m \u001B[38;5;124;03m`args` and `kwargs` can be None if this `Function` was created with an\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2710\u001B[0m \u001B[38;5;124;03m    shape relaxation retracing.\u001B[39;00m\n\u001B[1;32m   2711\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   2712\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_signature \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m args \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m kwargs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   2713\u001B[0m   args, kwargs, filtered_flat_args \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m-> 2714\u001B[0m       \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_function_spec\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcanonicalize_function_inputs\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m   2715\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   2716\u001B[0m   filtered_flat_args \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[0;32m~/gits/venv/lib/python3.10/site-packages/tensorflow/python/eager/function_spec.py:427\u001B[0m, in \u001B[0;36mFunctionSpec.canonicalize_function_inputs\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    424\u001B[0m       kwargs\u001B[38;5;241m.\u001B[39msetdefault(kwarg, default)\n\u001B[1;32m    426\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_signature \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 427\u001B[0m   inputs, flat_inputs, filtered_flat_inputs \u001B[38;5;241m=\u001B[39m \u001B[43m_convert_numpy_inputs\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    428\u001B[0m   kwargs, flat_kwargs, filtered_flat_kwargs \u001B[38;5;241m=\u001B[39m _convert_numpy_inputs(kwargs)\n\u001B[1;32m    429\u001B[0m   flat_inputs \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m flat_kwargs\n",
      "File \u001B[0;32m~/gits/venv/lib/python3.10/site-packages/tensorflow/python/eager/function_spec.py:471\u001B[0m, in \u001B[0;36m_convert_numpy_inputs\u001B[0;34m(inputs)\u001B[0m\n\u001B[1;32m    469\u001B[0m need_packing \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    470\u001B[0m filtered_flat_inputs \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m--> 471\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m index, value \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mflat_inputs\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m    472\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(value,\n\u001B[1;32m    473\u001B[0m                 (ops\u001B[38;5;241m.\u001B[39mTensor, resource_variable_ops\u001B[38;5;241m.\u001B[39mBaseResourceVariable)):\n\u001B[1;32m    474\u001B[0m     filtered_flat_inputs\u001B[38;5;241m.\u001B[39mappend(value)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from Baseline.HierarchicalHAR_PBD import build_model\n",
    "import Baseline.utils as utils\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import h5py\n",
    "import os\n",
    "from helper import merge_option_1, new_encoding\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "tf.keras.utils.set_random_seed(42) # Sets it for TF, numpy and base Python\n",
    "from tensorflow.keras.layers import * # for the new versions of Tensorflow, layers, models, regularizers, and optimizers shall be imported from Tensorflow.\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.regularizers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from keras.losses import * # and losses, metrics, callbacks, and backend can still be used from Keras directly.\n",
    "from keras.metrics import *\n",
    "from keras import metrics\n",
    "from sklearn.metrics import *\n",
    "from keras import backend as K\n",
    "from keras.backend import *\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import TensorBoard\n",
    "from numpy.linalg import inv\n",
    "from scipy.linalg import fractional_matrix_power\n",
    "from keras.utils.np_utils import *\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score,accuracy_score\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "adjacency_matrix = np.zeros((4, 4))\n",
    "adjacency_matrix[0, 1] = 1\n",
    "adjacency_matrix[1, 0] = 1\n",
    "adjacency_matrix[1, 2] = 1\n",
    "adjacency_matrix[1, 3] = 1\n",
    "adjacency_matrix[2, 1] = 1\n",
    "adjacency_matrix[2, 3] = 1\n",
    "adjacency_matrix[3, 1] = 1\n",
    "adjacency_matrix[3, 2] = 1\n",
    "print(\"Adjacency matrix: \", adjacency_matrix)\n",
    "norm_adj = utils.MakeGraph(adjacency_matrix)\n",
    "\n",
    "class HAR_model_wrapper():\n",
    "    \"A class to hold a HAR model and its important properties\"\n",
    "    timestep = 0\n",
    "    node_num = 0\n",
    "    feature_num = 0\n",
    "    adjacency_matrix = None\n",
    "    num_classes = 0\n",
    "    def __init__(self, adjacency_matrix, timestep, node_num, feature_num, num_class_HAR=26):\n",
    "        assert adjacency_matrix.shape[0] == node_num\n",
    "        assert adjacency_matrix.shape[1] == node_num\n",
    "        self.model = build_model(timestep=timestep, body_num=node_num, feature_dim=feature_num,\n",
    "                              gcn_units_HAR=26, lstm_units_HAR=24, adjacency_matrix=adjacency_matrix,\n",
    "                              gcn_units_PBD=16, lstm_units_PBD=24,\n",
    "                              num_class_HAR=num_class_HAR, num_class_PBD=2)[1]\n",
    "        self.num_classes = num_class_HAR\n",
    "        self.timestep = timestep\n",
    "        self.node_num = node_num\n",
    "        self.feature_num = feature_num\n",
    "        self.adjacency_matrix = adjacency_matrix\n",
    "\n",
    "\n",
    "\n",
    "X_train = np.load(\"Data/X_train_pain.npy\")\n",
    "Y_train = np.load(\"Data/Y_train_pain.npy\")\n",
    "X_test = np.load(\"Data/X_test_pain.npy\")\n",
    "Y_test = np.load(\"Data/Y_test_pain.npy\")\n",
    "# Option for merging. Make sure to call this before -1\n",
    "merge_option_1(Y_train)\n",
    "merge_option_1(Y_test)\n",
    "new_encoding(Y_train)\n",
    "new_encoding(Y_test)\n",
    "result = np.matmul(norm_adj, X_train[0, 0, :, :])\n",
    "# print(\"Result of matrix multiplication of normalized adjacency matrix with \"\n",
    "#       \"4x3 matrix from X[0, 0]: \", result,\n",
    "#       \"and its shape: \", result.shape)\n",
    "\n",
    "#Tensorflow expects classes to start from 0, otherwise it throws a fit\n",
    "Y_train = Y_train - 1\n",
    "Y_test = Y_test - 1\n",
    "print(\"Classes in Y_train: \", np.unique(Y_train))\n",
    "\n",
    "class_counts = np.unique(Y_train, return_counts=True)\n",
    "# Add missing labels to class_counts\n",
    "def add_missing_labels(class_counts: tuple) -> tuple:\n",
    "    labels = class_counts[0]\n",
    "    counts = class_counts[1]\n",
    "    new_labels = np.array(list(range(0, 26)))\n",
    "    # Assume that the labels with 0 samples have 1 sample\n",
    "    # to avoid a divide by zero error\n",
    "    # the effect of this assumption is quite negligible\n",
    "    new_counts = np.ones(shape=(26,))\n",
    "    print(new_counts)\n",
    "    res = {new_labels[i]: new_counts[i] for i in range(len(new_labels))}\n",
    "    for i in range(len(labels)):\n",
    "        res[labels[i]] = counts[i]\n",
    "    return res\n",
    "\n",
    "# Now 27 labels\n",
    "HARmodel = HAR_model_wrapper(adjacency_matrix=adjacency_matrix,\n",
    "                             timestep=120, node_num=4, feature_num=3, num_class_HAR=27)\n",
    "\n",
    "\n",
    "def train_model(model: HAR_model_wrapper, X_train: np.ndarray, X_test: np.ndarray,\n",
    "                Y_train: np.ndarray, Y_test: np.ndarray):\n",
    "    AdjNorm = utils.MakeGraph(model.adjacency_matrix)\n",
    "    graphtrain = utils.my_combine(AdjNorm, X_train)\n",
    "    graphtest = utils.my_combine(AdjNorm, X_test)\n",
    "    # print(\"Shape of X train :\", X_train.shape)\n",
    "    # print(\"Shape of Y train before one-hot encoding: \", Y_train.shape)\n",
    "    class_counts = np.unique(Y_train, return_counts=True)\n",
    "    class_counts = add_missing_labels(class_counts)\n",
    "    print(\"Class counts: \", class_counts)\n",
    "    # One hot encoding\n",
    "    Y_train = to_categorical(Y_train, num_classes=model.num_classes)\n",
    "    Y_test = to_categorical(Y_test, num_classes=model.num_classes)\n",
    "    # print(\"Shape of categorically encoded Y_train: \", Y_train.shape)\n",
    "    # print(\"Shape of categorically encoded Y_test: \", Y_test.shape)\n",
    "    # Beta = 0.9999 produces a really small loss\n",
    "    model.model.compile(optimizer=Adam(learning_rate=5e-4, decay=1e-5),\n",
    "                  loss={\n",
    "                        # 'HARout': 'categorical_crossentropy'\n",
    "                        'HARout': utils.focal_loss(weights = utils.class_balance_weights(0.30,\n",
    "                                     list(class_counts.values())),\n",
    "                                     gamma=5, num_class=model.num_classes)\n",
    "                        },\n",
    "                  loss_weights={'HARout': 1.},\n",
    "                  metrics=['categorical_accuracy'])\n",
    "\n",
    "    model.model.fit(x=graphtrain,\n",
    "              y=Y_train,\n",
    "              batch_size=150,\n",
    "              epochs=100,\n",
    "              #callbacks=utils.build_callbacks('Model', str(valid_patient)),\n",
    "              validation_data=(graphtest, Y_test)\n",
    "              )\n",
    "    model.model.save(\"Models/GC_LSTM_HAR\")\n",
    "    return model.model\n",
    "\n",
    "train = input(\"Train model? Yes/no\")\n",
    "if train==\"Yes\":\n",
    "    model = train_model(HARmodel, X_train, X_test, Y_train, Y_test)\n",
    "else:\n",
    "    print(\"Loading model…\")\n",
    "    model = keras.models.load_model(\"Models/GC_LSTM_HAR\")\n",
    "AdjNorm = utils.MakeGraph(HARmodel.adjacency_matrix)\n",
    "graphtest = utils.my_combine(AdjNorm, X_test)\n",
    "print(\"Y test before categorical encoding: \", Y_test.shape)\n",
    "predictions = model.predict(graphtest)\n",
    "print(\"Shape of predictions: \", predictions.shape)\n",
    "print(\"First predictions: \", predictions[0])\n",
    "# Do these numbers actually sum to 1?\n",
    "for i in range(predictions.shape[0]):\n",
    "    print(\"Sum: \", np.sum(predictions[0]))\n",
    "    break\n",
    "\n",
    "#Pretty much. So pick the most likely class\n",
    "def zeros_and_ones(arr):\n",
    "    to_return = np.zeros(shape=arr.shape[0])\n",
    "    for i in range(arr.shape[0]):\n",
    "        # print(\"Index of biggest number: \", np.argmax(arr[i]))\n",
    "        to_return[i] = np.argmax(arr[i])\n",
    "    return to_return\n",
    "\n",
    "# Transform predictions back to original shape\n",
    "predictions = zeros_and_ones(predictions)\n",
    "# Save results\n",
    "results = {\"F1 score\": f1_score(Y_test, predictions, average='weighted'),\n",
    "           \"Accuracy\": accuracy_score(Y_test, predictions),\n",
    "           \"Precision\": precision_score(Y_test, predictions, average='weighted'),\n",
    "           \"Recall\": recall_score(Y_test, predictions, average='weighted'),\n",
    "           \"Confusion matrix\": confusion_matrix(Y_test, predictions, labels=np.array(list(range(1, 27))))}\n",
    "print(\"F1 score \", f1_score(Y_test, predictions, average='weighted'))\n",
    "print(\"Accuracy \", accuracy_score(Y_test, predictions))\n",
    "print(\"Precision \", precision_score(Y_test, predictions, average='weighted'))\n",
    "print(\"Recall \", recall_score(Y_test, predictions, average='weighted'))\n",
    "name = input(\"Please name this experiment\")\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "json.dump(results, open(\"Results/Experiment_\" + name, \"w\"), cls=NumpyEncoder)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}