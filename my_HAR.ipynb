{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjacency matrix:  [[0. 1. 0. 0.]\n",
      " [1. 0. 1. 1.]\n",
      " [0. 1. 0. 1.]\n",
      " [0. 1. 1. 0.]]\n",
      "Classes in Y_train:  [ 0.  1.  2.  3.  4.  5.  6.  8. 13. 16. 17. 20. 21. 22. 23. 24. 25. 26.]\n",
      "Timestep:  120 Body num:  4 Num classes HAR:  18\n",
      "HARextend shape:  (None, 120, 4, 18)\n",
      "HAR temporal output1 shape:  (None, 18)\n",
      "PDB temporal output1 shape  (None, 2)\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1.]\n",
      "Class counts:  {0: 4104, 1: 2763, 2: 3972, 3: 198, 4: 690, 5: 291, 6: 378, 7: 1.0, 8: 3747, 9: 1.0, 10: 1.0, 11: 1.0, 12: 1.0, 13: 1284, 14: 1.0, 15: 1.0, 16: 1254, 17: 114, 18: 1.0, 19: 1.0, 20: 4962, 21: 219, 22: 795, 23: 5007, 24: 657, 25: 756, 26.0: 579}\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 20 is out of bounds for axis 1 with size 18",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [1], line 144\u001B[0m\n\u001B[1;32m    142\u001B[0m train \u001B[38;5;241m=\u001B[39m \u001B[38;5;28minput\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTrain model? Yes/no\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    143\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m train\u001B[38;5;241m==\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYes\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 144\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mHARmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY_test\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    145\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    146\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLoading model…\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn [1], line 117\u001B[0m, in \u001B[0;36mtrain_model\u001B[0;34m(model, X_train, X_test, Y_train, Y_test)\u001B[0m\n\u001B[1;32m    115\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mClass counts: \u001B[39m\u001B[38;5;124m\"\u001B[39m, class_counts)\n\u001B[1;32m    116\u001B[0m \u001B[38;5;66;03m# One hot encoding\u001B[39;00m\n\u001B[0;32m--> 117\u001B[0m Y_train \u001B[38;5;241m=\u001B[39m \u001B[43mto_categorical\u001B[49m\u001B[43m(\u001B[49m\u001B[43mY_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_classes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_classes\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    118\u001B[0m Y_test \u001B[38;5;241m=\u001B[39m to_categorical(Y_test, num_classes\u001B[38;5;241m=\u001B[39mmodel\u001B[38;5;241m.\u001B[39mnum_classes)\n\u001B[1;32m    119\u001B[0m \u001B[38;5;66;03m# print(\"Shape of categorically encoded Y_train: \", Y_train.shape)\u001B[39;00m\n\u001B[1;32m    120\u001B[0m \u001B[38;5;66;03m# print(\"Shape of categorically encoded Y_test: \", Y_test.shape)\u001B[39;00m\n\u001B[1;32m    121\u001B[0m \u001B[38;5;66;03m# Beta = 0.9999 produces a really small loss\u001B[39;00m\n",
      "File \u001B[0;32m~/gits/venv/lib/python3.10/site-packages/keras/utils/np_utils.py:73\u001B[0m, in \u001B[0;36mto_categorical\u001B[0;34m(y, num_classes, dtype)\u001B[0m\n\u001B[1;32m     71\u001B[0m n \u001B[38;5;241m=\u001B[39m y\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m     72\u001B[0m categorical \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mzeros((n, num_classes), dtype\u001B[38;5;241m=\u001B[39mdtype)\n\u001B[0;32m---> 73\u001B[0m \u001B[43mcategorical\u001B[49m\u001B[43m[\u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marange\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m     74\u001B[0m output_shape \u001B[38;5;241m=\u001B[39m input_shape \u001B[38;5;241m+\u001B[39m (num_classes,)\n\u001B[1;32m     75\u001B[0m categorical \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mreshape(categorical, output_shape)\n",
      "\u001B[0;31mIndexError\u001B[0m: index 20 is out of bounds for axis 1 with size 18"
     ]
    }
   ],
   "source": [
    "from Baseline.HierarchicalHAR_PBD import build_model\n",
    "import Baseline.utils as utils\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import h5py\n",
    "import os\n",
    "from helper import merge_option_1, merge_option_2\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "tf.keras.utils.set_random_seed(42) # Sets it for TF, numpy and base Python\n",
    "from tensorflow.keras.layers import * # for the new versions of Tensorflow, layers, models, regularizers, and optimizers shall be imported from Tensorflow.\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.regularizers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from keras.losses import * # and losses, metrics, callbacks, and backend can still be used from Keras directly.\n",
    "from keras.metrics import *\n",
    "from keras import metrics\n",
    "from sklearn.metrics import *\n",
    "from keras import backend as K\n",
    "from keras.backend import *\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import TensorBoard\n",
    "from numpy.linalg import inv\n",
    "from scipy.linalg import fractional_matrix_power\n",
    "from keras.utils.np_utils import *\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score,accuracy_score\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "adjacency_matrix = np.zeros((4, 4))\n",
    "adjacency_matrix[0, 1] = 1\n",
    "adjacency_matrix[1, 0] = 1\n",
    "adjacency_matrix[1, 2] = 1\n",
    "adjacency_matrix[1, 3] = 1\n",
    "adjacency_matrix[2, 1] = 1\n",
    "adjacency_matrix[2, 3] = 1\n",
    "adjacency_matrix[3, 1] = 1\n",
    "adjacency_matrix[3, 2] = 1\n",
    "print(\"Adjacency matrix: \", adjacency_matrix)\n",
    "norm_adj = utils.MakeGraph(adjacency_matrix)\n",
    "\n",
    "class HAR_model_wrapper():\n",
    "    \"A class to hold a HAR model and its important properties\"\n",
    "    timestep = 0\n",
    "    node_num = 0\n",
    "    feature_num = 0\n",
    "    adjacency_matrix = None\n",
    "    num_classes = 0\n",
    "    def __init__(self, adjacency_matrix, timestep, node_num, feature_num, num_class_HAR=26):\n",
    "        assert adjacency_matrix.shape[0] == node_num\n",
    "        assert adjacency_matrix.shape[1] == node_num\n",
    "        self.model = build_model(timestep=timestep, body_num=node_num, feature_dim=feature_num,\n",
    "                              gcn_units_HAR=26, lstm_units_HAR=24, adjacency_matrix=adjacency_matrix,\n",
    "                              gcn_units_PBD=16, lstm_units_PBD=24,\n",
    "                              num_class_HAR=num_class_HAR, num_class_PBD=2)[1]\n",
    "        self.num_classes = num_class_HAR\n",
    "        self.timestep = timestep\n",
    "        self.node_num = node_num\n",
    "        self.feature_num = feature_num\n",
    "        self.adjacency_matrix = adjacency_matrix\n",
    "\n",
    "\n",
    "\n",
    "X_train = np.load(\"Data/X_train_full.npy\")\n",
    "Y_train = np.load(\"Data/Y_train_full.npy\")\n",
    "X_test = np.load(\"Data/X_test_full.npy\")\n",
    "Y_test = np.load(\"Data/Y_test_full.npy\")\n",
    "# Option for merging. Make sure to call this before -1\n",
    "merge_option_1(Y_train)\n",
    "merge_option_1(Y_test)\n",
    "result = np.matmul(norm_adj, X_train[0, 0, :, :])\n",
    "# print(\"Result of matrix multiplication of normalized adjacency matrix with \"\n",
    "#       \"4x3 matrix from X[0, 0]: \", result,\n",
    "#       \"and its shape: \", result.shape)\n",
    "\n",
    "#Tensorflow expects classes to start from 0, otherwise it throws a fit\n",
    "Y_train = Y_train - 1\n",
    "Y_test = Y_test - 1\n",
    "print(\"Classes in Y_train: \", np.unique(Y_train))\n",
    "\n",
    "class_counts = np.unique(Y_train, return_counts=True)\n",
    "# Add missing labels to class_counts\n",
    "def add_missing_labels(class_counts: tuple) -> tuple:\n",
    "    labels = class_counts[0]\n",
    "    counts = class_counts[1]\n",
    "    new_labels = np.array(list(range(0, 26)))\n",
    "    # Assume that the labels with 0 samples have 1 sample\n",
    "    # to avoid a divide by zero error\n",
    "    # the effect of this assumption is quite negligible\n",
    "    new_counts = np.ones(shape=(26,))\n",
    "    print(new_counts)\n",
    "    res = {new_labels[i]: new_counts[i] for i in range(len(new_labels))}\n",
    "    for i in range(len(labels)):\n",
    "        res[labels[i]] = counts[i]\n",
    "    return res\n",
    "\n",
    "# Now 27 labels\n",
    "HARmodel = HAR_model_wrapper(adjacency_matrix=adjacency_matrix,\n",
    "                             timestep=120, node_num=4, feature_num=3, num_class_HAR=27)\n",
    "\n",
    "\n",
    "def train_model(model: HAR_model_wrapper, X_train: np.ndarray, X_test: np.ndarray,\n",
    "                Y_train: np.ndarray, Y_test: np.ndarray):\n",
    "    AdjNorm = utils.MakeGraph(model.adjacency_matrix)\n",
    "    graphtrain = utils.my_combine(AdjNorm, X_train)\n",
    "    graphtest = utils.my_combine(AdjNorm, X_test)\n",
    "    # print(\"Shape of X train :\", X_train.shape)\n",
    "    # print(\"Shape of Y train before one-hot encoding: \", Y_train.shape)\n",
    "    class_counts = np.unique(Y_train, return_counts=True)\n",
    "    class_counts = add_missing_labels(class_counts)\n",
    "    print(\"Class counts: \", class_counts)\n",
    "    # One hot encoding\n",
    "    Y_train = to_categorical(Y_train, num_classes=model.num_classes)\n",
    "    Y_test = to_categorical(Y_test, num_classes=model.num_classes)\n",
    "    # print(\"Shape of categorically encoded Y_train: \", Y_train.shape)\n",
    "    # print(\"Shape of categorically encoded Y_test: \", Y_test.shape)\n",
    "    # Beta = 0.9999 produces a really small loss\n",
    "    model.model.compile(optimizer=Adam(learning_rate=5e-4, decay=1e-5),\n",
    "                  loss={\n",
    "                        # 'HARout': 'categorical_crossentropy'\n",
    "                        'HARout': utils.focal_loss(weights = utils.class_balance_weights(0.30,\n",
    "                                     list(class_counts.values())),\n",
    "                                     gamma=5, num_class=model.num_classes)\n",
    "                        },\n",
    "                  loss_weights={'HARout': 1.},\n",
    "                  metrics=['categorical_accuracy'])\n",
    "\n",
    "    model.model.fit(x=graphtrain,\n",
    "              y=Y_train,\n",
    "              batch_size=150,\n",
    "              epochs=100,\n",
    "              #callbacks=utils.build_callbacks('Model', str(valid_patient)),\n",
    "              validation_data=(graphtest, Y_test)\n",
    "              )\n",
    "    model.model.save(\"Models/GC_LSTM_HAR\")\n",
    "    return model.model\n",
    "\n",
    "train = input(\"Train model? Yes/no\")\n",
    "if train==\"Yes\":\n",
    "    model = train_model(HARmodel, X_train, X_test, Y_train, Y_test)\n",
    "else:\n",
    "    print(\"Loading model…\")\n",
    "    model = keras.models.load_model(\"Models/GC_LSTM_HAR\")\n",
    "AdjNorm = utils.MakeGraph(HARmodel.adjacency_matrix)\n",
    "graphtest = utils.my_combine(AdjNorm, X_test)\n",
    "print(\"Y test before categorical encoding: \", Y_test.shape)\n",
    "predictions = model.predict(graphtest)\n",
    "print(\"Shape of predictions: \", predictions.shape)\n",
    "print(\"First predictions: \", predictions[0])\n",
    "# Do these numbers actually sum to 1?\n",
    "for i in range(predictions.shape[0]):\n",
    "    print(\"Sum: \", np.sum(predictions[0]))\n",
    "    break\n",
    "\n",
    "#Pretty much. So pick the most likely class\n",
    "def zeros_and_ones(arr):\n",
    "    to_return = np.zeros(shape=arr.shape[0])\n",
    "    for i in range(arr.shape[0]):\n",
    "        # print(\"Index of biggest number: \", np.argmax(arr[i]))\n",
    "        to_return[i] = np.argmax(arr[i])\n",
    "    return to_return\n",
    "\n",
    "# Transform predictions back to original shape\n",
    "predictions = zeros_and_ones(predictions)\n",
    "# Save results\n",
    "results = {\"F1 score\": f1_score(Y_test, predictions, average='weighted'),\n",
    "           \"Accuracy\": accuracy_score(Y_test, predictions),\n",
    "           \"Precision\": precision_score(Y_test, predictions, average='weighted'),\n",
    "           \"Recall\": recall_score(Y_test, predictions, average='weighted'),\n",
    "           \"Confusion matrix\": confusion_matrix(Y_test, predictions, labels=np.array(list(range(1, 27))))}\n",
    "print(\"F1 score \", f1_score(Y_test, predictions, average='weighted'))\n",
    "print(\"Accuracy \", accuracy_score(Y_test, predictions))\n",
    "print(\"Precision \", precision_score(Y_test, predictions, average='weighted'))\n",
    "print(\"Recall \", recall_score(Y_test, predictions, average='weighted'))\n",
    "name = input(\"Please name this experiment\")\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "json.dump(results, open(\"Results/Experiment_\" + name, \"w\"), cls=NumpyEncoder)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}